---
title: "Final Project"
author: "Bruce Mallory"
date: "4/10/2021"
output:
  html_document:
    df_print: paged
---

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(stringr)
library(tidytext)
library(lubridate)
library(ggplot2)
library(gridExtra)
library(tidyr)
library(sentimentr)
library(textdata)
library(scales)
library(viridis)
library(wordcloud)
library(RColorBrewer)

setwd("~/MSSP/747 Social Data Analysis/747_FinalProject")
```

## A. Downloading and preparing data

I started with Nexis/Uni and searched for:
  "For The People Act",
  News,
  English,
  The New York Times,
  <dates>
  
Because of the limit on the number of articles that Nexis/Uni allowed me to download, I downloaded articles in two month intervals (in .RTF format).  Before downloading, I went through the list of articles that Nexis/Uni generated and removed the duplicate articles.  For the New York Times (and I imagine many other news organizations) slightly modified versions of the same article are published in different newspaper releases, and as such the same article could show up several times in the Nexis/Uni search.  Many times, I found that it was just the title that had changed when the article was republished.

For each of the .RTF files I converted them to .txt files (so I could modify them), added "Document-Type:" where it was missing (mostly for National Desk articles that weren't given a "Document-Type:" tag), removed the explanatory verbiage at the end of the article (e.g. PHOTO captions, author descriptions, live links), removed summary articles of Late Night scripts (of which there were many), removed all articles that were daily briefing (which summarized the day's articles), and removed duplicate articles that had gotten through my Nexis/Uni filtering.


## B1. Importing Data and Metric Calculation: functions

I wrote functions to:
  (1) load chunks of articles (from the .txt files), tokenize by word, and calculate word frequencies,
  (2) calculate article sentiments using multiple sentiment dictionaries

```{r message=FALSE, warning=FALSE}
source("wrangling_functions.R")
```

## B2. Importing and Measuring Data: building the working data.frames ("Article_sntmts" & "Article_wdfrq")

```{r message=FALSE, warning=FALSE}
#Code to load the chunks of articles into my working data.frame. 

data("stop_words")

#Global variable to number the articles
y <- 1

Article_wdfrq <- initialize_wdfrq_df()

Article_wdfrq <- rbind(Article_wdfrq, load_articles("Data/Nov_Dec.txt"))
y <- max(Article_wdfrq$Article)+1
Article_wdfrq <- rbind(Article_wdfrq, load_articles("Data/Jan_Feb.txt"))
y <- max(Article_wdfrq$Article)+1
Article_wdfrq <- rbind(Article_wdfrq, load_articles("Data/Mar_Apr.txt"))
y <- max(Article_wdfrq$Article)+1
Article_wdfrq <- rbind(Article_wdfrq, load_articles("Data/May_June.txt"))
y <- max(Article_wdfrq$Article)+1
Article_wdfrq <- rbind(Article_wdfrq, load_articles("Data/July_Aug.txt"))
y <- max(Article_wdfrq$Article)+1

#Code to loop through all the articles, using measure_sentiment() to collect each article's sentiment.

Article_sntmts <- initialize_sntmts_df()

for (i in 1:max(Article_wdfrq$Article)) {
  Article_sntmts <-
    rbind(Article_sntmts, measure_sentiment(i))
}

#Then going back to the Article_wdfrq data.frame to get the title sentiments, and joining them to the Article_sntmts data.frame.

title_scores <- get_sentences(unique(Article_wdfrq$title)) %>% 
  sentiment() %>% 
  group_by(element_id) %>% 
  summarise(score = sum(sentiment)) %>% 
  rename(Article=element_id, title_score=score)

Article_sntmts <- full_join(Article_sntmts, title_scores, by="Article")


#Saving the running of the previous lines back into data.frames so that I can reload the data.frames without having to run this chunk of code again.  I did this for the process of creating my data display functions, and collecting interesting results.
#save(Article_wdfrq, file = "Data/Article_wdfrq.RData")
#save(Article_sntmts, file = "Data/Article_sntmts.RData")
#load(file = "Data/Article_wdfrq.RData")
#load(file = "Data/Article_sntmts.RData")

#Code to help me verify that my functions are getting the correct data in the correct places.  
#Article_titles <- Article_wdfrq[!duplicated(Article_wdfrq$Article),]

```
## C1. Finding focus words: functions

I am writing a function to display a bar graph of the 30 most frequent words in articles in the top and bottom quartiles of articles based on a given sentiment_score

```{r message=FALSE, warning=FALSE}
source("filtering_functions.R")
```

## C2. Finding focus words: visualizations
Two visualizations to help find interesting words to track:
(1) a wordcloud of all the words in all of the articles 
(2) frequency bar graph of the top 30 words contributing to pos/neg sentiment_scores
```{r}

#Code to give me the most frequently used words in the articles.
# words <- Article_wdfrq %>% group_by(word) %>% summarise(n=sum(n))
# words <- words[order(words$n, decreasing=TRUE),]

words_used <- Article_wdfrq %>% group_by(word) %>%
  summarise(total=sum(n)) %>% 
  arrange(desc(total))

wordcloud(words = words_used$word, freq = words_used$total, min.freq = 50,           
          max.words=200, random.order=FALSE, rot.per=0.35, 
          scale=c(3.5,0.25), colors=brewer.pal(8, "Dark2"))

pos_scores <- left_join(Article_wdfrq, Article_sntmts, by=c("Article", "Type", "Date")) %>% 
  select(word, Article, Type, Date, n, afinn_score) %>% 
  filter(afinn_score>quantile(afinn_score)[4]) %>% 
  group_by(word) %>%
  summarise(total=sum(n)) %>% 
  arrange(desc(total))

wordcloud(words = pos_scores$word, freq = pos_scores$total, min.freq = 25,           
          max.words=200, random.order=FALSE, rot.per=0.35, 
          scale=c(3.5,0.25), colors=brewer.pal(8, "Dark2"))

neg_scores <- left_join(Article_wdfrq, Article_sntmts, by=c("Article", "Type", "Date")) %>% 
  select(word, Article, Type, Date, n, afinn_score) %>% 
  filter(afinn_score<quantile(afinn_score)[2]) %>% 
  group_by(word) %>%
  summarise(total=sum(n)) %>% 
  arrange(desc(total))

wordcloud(words = neg_scores$word, freq = neg_scores$total, min.freq = 25,           
          max.words=200, random.order=FALSE, rot.per=0.35, 
          scale=c(3.5,0.25), colors=brewer.pal(8, "Dark2"))

freq_wrds(afinn_score)
```

## D. Displaying data: functions

I wrote functions to:
(1) display word frequency over time - with key events overlayed.
This function takes two inputs:
a) the type of article ("Op-Ed", "Editorial", "Letter", and/or "News")
b) the word whose frequency within each article is being displayed

(2) display article sentiments over time - with key events overlayed.
This function takes three inputs:
a) the type of article (either "Op-Ed", "Editorial", "Letter", and/or "News")
b) the sentiment score to be used
c) a word whose frequency within each article will be displayed


```{r}
source("display_functions.R")
```

## E3. Displaying data: intersting results

NOTE: In tokenizing the capitalization is dropped, and as such I've lost the ability to distinguish between captital-D "Democratic" - e.g. Democratic policy, versus small-d "democratic" - e.g. democratic ideals.
```{r  message=FALSE, warning=FALSE, message=FALSE}

wdfrq_over_time(c("News"),"justice")
wdfrq_over_time(c("Op-Ed", "Editorial"),"manchin")
wdfrq_over_time(c("Op-Ed", "Editorial", "Letter"),"manchin")

sntmts_over_time(c("Op-Ed", "Editorial", "Letter"), "nrc_fear_score", "sinema")


```
