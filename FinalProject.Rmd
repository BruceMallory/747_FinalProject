---
title: "Final Project"
author: "Bruce Mallory"
date: "4/10/2021"
output: pdf_document
---

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(stringr)
library(tidytext)
library(lubridate)
library(ggplot2)
library(tidyr)
library(sentimentr)
library(textdata)
library(scales)

setwd("~/MSSP/747 Social Data Analysis/747_FinalProject")
```

## A. Downloading and preparing data

I started with Nexis/Uni and searched for:
  "For The People Act",
  News,
  English,
  The New York Times,
  <dates>
  
Because of the limit on the number of articles that Nexis/Uni allowed me to download, I downloaded articles in two month intervals (in .RTF format).  Before downloading, I went through the list of articles that Nexis/Uni generated and removed the duplicate articles.  For the New York Times (and I imagine many other news organizations) slightly modified versions of the same article are published in different newspaper releases, and as such the same article could show up several times in the Nexis/Uni search.  Many times, I found that it was just the title that had changed when the article was republished.

For each of the .RTF files I converted them to .txt files (so I could modify them), added "Document-Typ:" where it was missing (mostly for National Desk articles that weren't given a "Document-Type:" tag), removed the explanatory verbiage at the end of the article (e.g. PHOTO captions, author descriptions, live links), removed summary articles of Late Night scripts (of which there were many), removed all articles that were daily briefing (which summarized the day's articles), and removed duplicate articles that had gotten through my Nexis/Uni filtering.


## B1. Importing and Measuring Data: functions

I wrote functions to:
  (1) load chunks of articles (from the .txt files), tokenize by word, and calculate word frequencies,
  (2) calculate word sentiments

```{r message=FALSE, warning=FALSE}
source("wrangling_functions.R")
```

## B2. Importing and Measuring Data: building the working data.frames ("All-articles" & "Article_sntmts")

```{r message=FALSE, warning=FALSE}
#Code to load the chunks of articles into my working data.frame. 

data("stop_words")

#Global variable to number the articles
y <- 1

Article_wdfrq <- initialize_wdfrq_df()

Article_wdfrq <- rbind(Article_wdfrq, load_articles("Data/Nov_Dec.txt"))
y <- max(Article_wdfrq$Article)+1
Article_wdfrq <- rbind(Article_wdfrq, load_articles("Data/Jan_Feb.txt"))
y <- max(Article_wdfrq$Article)+1
Article_wdfrq <- rbind(Article_wdfrq, load_articles("Data/Mar_Apr.txt"))
y <- max(Article_wdfrq$Article)+1
Article_wdfrq <- rbind(Article_wdfrq, load_articles("Data/May_June.txt"))
y <- max(Article_wdfrq$Article)+1
Article_wdfrq <- rbind(Article_wdfrq, load_articles("Data/July_Aug.txt"))
y <- max(Article_wdfrq$Article)+1

#Code to loop through all the articles, using measure_sentiment() to collect each article's sentiment.

Article_sntmts <- initialize_sntmts_df()

for (i in 1:max(Article_wdfrq$Article)) {
  Article_sntmts <-
    rbind(Article_sntmts, measure_sentiment(i))
}

#Then going back to the Article_wdfrq data.frame to get the title sentiments, and joining them to the Article_sntmts data.frame.

title_scores <- get_sentences(unique(Article_wdfrq$title)) %>% 
  sentiment() %>% 
  group_by(element_id) %>% 
  summarise(score = sum(sentiment)) %>% 
  rename(Article=element_id, title_score=score)

Article_sntmts <- full_join(Article_sntmts, title_scores, by="Article")


#Saving the running of the previous lines back into data.frames so that I can reload the data.frames without having to run this chunk of code again.  I did this for the process of creating my data display functions, and collecting interesting results.
#save(Article_wdfrq, file = "Data/Article_wdfrq.RData")
#save(Article_sntmts, file = "Data/Article_sntmts.RData")
#load(file = "Data/Article_wdfrq.RData")
#load(file = "Data/Article_sntmts.RData")

#Code to help me verify that my functions are getting the correct data in the correct places.  
#Article_titles <- Article_wdfrq[!duplicated(Article_wdfrq$Article),]

```

## C1. Displaying data: functions

I wrote functions to:
(1) display word frequency over time - with key events overlayed,
(2) display article sentiments over time - with key events overlayed.

```{r}
source("display_functions.R")
```

## C2. Displaying data: intersting results

NOTE: In tokenizing the capitalization is dropped, and as such I've lost the ability to distinguish between captital-D "Democratic" - e.g. Democratic policy, versus small-d "democratic" - e.g. democratic ideals.
```{r  message=FALSE, warning=FALSE}

#Code to give me the most frequently used words in the articles.
# words <- Article_wdfrq %>% group_by(word) %>% summarise(n=sum(n))
# words <- words[order(words$n, decreasing=TRUE),]

wdfrq_over_time(c("News"),"justice")
wdfrq_over_time(c("Op-Ed", "Editorial"),"manchin")
wdfrq_over_time(c("Op-Ed", "Editorial", "Letter"),"manchin")


```
